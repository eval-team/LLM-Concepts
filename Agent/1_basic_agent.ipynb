{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab1fb05",
   "metadata": {},
   "source": [
    "# Basic Agent with langchain\n",
    "\n",
    "\n",
    "* Conversational Agent with Context Awareness\n",
    "* Question Answering Agent\n",
    "* Data Analysis Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f364cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -q langchain langchain_experimental openai python-dotenv langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726d5d8",
   "metadata": {},
   "source": [
    "## Conversational Agent with Context Awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9356763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now get the API key (it should be loaded from .env file)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "else:\n",
    "    print(\"Warning: OPENAI_API_KEY not found in environment variables.\")\n",
    "    print(\"Please make sure you have a .env file with OPENAI_API_KEY=your_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ea36a",
   "metadata": {},
   "source": [
    "### Initialize language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cc76f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c0bb8",
   "metadata": {},
   "source": [
    "### in-memory store for chat histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b39d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_chat_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072d68c",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d60a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd945be",
   "metadata": {},
   "source": [
    "### Combine the prompt and model into a runnable chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b8cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481872ec",
   "metadata": {},
   "source": [
    "### Wrap the chain with message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce07cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc33115",
   "metadata": {},
   "source": [
    "### Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce84ac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n",
      "AI: Your previous message was, \"Hello! How are you?\" How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "session_id = \"user_123\"\n",
    "\n",
    "\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Hello! How are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"AI:\", response1.content)\n",
    "\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What was my previous message?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"AI:\", response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebcca8a",
   "metadata": {},
   "source": [
    "### Print the conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88d1b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation History:\n",
      "human: Hello! How are you?\n",
      "ai: Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n",
      "human: What was my previous message?\n",
      "ai: Your previous message was, \"Hello! How are you?\" How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConversation History:\")\n",
    "for message in store[session_id].messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb849d6",
   "metadata": {},
   "source": [
    "## Question Answering Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e2813",
   "metadata": {},
   "source": [
    "We will use the same model and but prompt template inste of ChatPromptTemplate.\n",
    "\n",
    "* For completion-style LLMs. we can use prompt template with placeholders, and fill them in at runtime.\n",
    "\n",
    "* For chat-based LLMs, we use ChatPromptTemplate with messages. Consists of a sequence of messages (SystemMessage, HumanMessage, AIMessage, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92bd4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4fcb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful AI assistant. Your task is to answer the user's question to the best of your ability.\n",
    "\n",
    "User's question: {question}\n",
    "\n",
    "Please provide a clear and concise answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd7049cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8992333",
   "metadata": {},
   "source": [
    "##### Define the get_answer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a701676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question):\n",
    "    \"\"\"\n",
    "    Get an answer to the given question using the QA chain.\n",
    "    \"\"\"\n",
    "    input_variables = {\"question\": question}\n",
    "    response = qa_chain.invoke(input_variables).content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a0ca6",
   "metadata": {},
   "source": [
    "##### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e40a258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Answer: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "answer = get_answer(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5aef16",
   "metadata": {},
   "source": [
    "## Data Analysis Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "712d8bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain.agents import AgentType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d5e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate sample data\n",
    "# n_rows = 1000\n",
    "\n",
    "# # Generate dates\n",
    "# start_date = datetime(2022, 1, 1)\n",
    "# dates = [start_date + timedelta(days=i) for i in range(n_rows)]\n",
    "\n",
    "# # Define data categories\n",
    "# makes = ['Toyota', 'Honda', 'Ford', 'Chevrolet', 'Nissan', 'BMW', 'Mercedes', 'Audi', 'Hyundai', 'Kia']\n",
    "# models = ['Sedan', 'SUV', 'Truck', 'Hatchback', 'Coupe', 'Van']\n",
    "# colors = ['Red', 'Blue', 'Black', 'White', 'Silver', 'Gray', 'Green']\n",
    "\n",
    "# # Create the dataset\n",
    "# data = {\n",
    "#     'Date': dates,\n",
    "#     'Make': np.random.choice(makes, n_rows),\n",
    "#     'Model': np.random.choice(models, n_rows),\n",
    "#     'Color': np.random.choice(colors, n_rows),\n",
    "#     'Year': np.random.randint(2015, 2023, n_rows),\n",
    "#     'Price': np.random.uniform(20000, 80000, n_rows).round(2),\n",
    "#     'Mileage': np.random.uniform(0, 100000, n_rows).round(0),\n",
    "#     'EngineSize': np.random.choice([1.6, 2.0, 2.5, 3.0, 3.5, 4.0], n_rows),\n",
    "#     'FuelEfficiency': np.random.uniform(20, 40, n_rows).round(1),\n",
    "#     'SalesPerson': np.random.choice(['Alice', 'Bob', 'Charlie', 'David', 'Eva'], n_rows)\n",
    "# }\n",
    "\n",
    "# # Create DataFrame and sort by date\n",
    "# df = pd.DataFrame(data).sort_values('Date')\n",
    "\n",
    "# # Save to CSV file\n",
    "# csv_filename = \"car_sales_data.csv\"\n",
    "# df.to_csv(csv_filename, index=False)\n",
    "# print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "# # Display sample data and statistics\n",
    "# print(\"\\nFirst few rows of the generated data:\")\n",
    "# print(df.head())\n",
    "\n",
    "# print(\"\\nDataFrame info:\")\n",
    "# df.info()\n",
    "\n",
    "# print(\"\\nSummary statistics:\")\n",
    "# print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db451e5f",
   "metadata": {},
   "source": [
    "### Load data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "febf1a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (1000, 10)\n",
      "\n",
      "First few rows of loaded data:\n",
      "        Date      Make      Model   Color  Year     Price  Mileage  \\\n",
      "0 2022-01-01  Mercedes      Sedan    Blue  2020  43639.25  65879.0   \n",
      "1 2022-01-02    Toyota  Hatchback    Gray  2016  44983.53  62313.0   \n",
      "2 2022-01-03   Hyundai        Van     Red  2019  47054.44  49459.0   \n",
      "3 2022-01-04    Toyota      Sedan    Blue  2020  69614.21  21126.0   \n",
      "4 2022-01-05    Nissan      Coupe  Silver  2018  25880.32  38198.0   \n",
      "\n",
      "   EngineSize  FuelEfficiency SalesPerson  \n",
      "0         1.6            26.6       David  \n",
      "1         3.5            23.4         Eva  \n",
      "2         2.5            29.3         Bob  \n",
      "3         3.0            22.0         Bob  \n",
      "4         4.0            26.7         Bob  \n",
      "\n",
      "Data types:\n",
      "Date              datetime64[ns]\n",
      "Make                      object\n",
      "Model                     object\n",
      "Color                     object\n",
      "Year                       int64\n",
      "Price                    float64\n",
      "Mileage                  float64\n",
      "EngineSize               float64\n",
      "FuelEfficiency           float64\n",
      "SalesPerson               object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('car_sales_data.csv')\n",
    "\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nFirst few rows of loaded data:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7044d",
   "metadata": {},
   "source": [
    "##### Create Data Analysis Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d3cfaf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\source\\LLM-Concepts\\.venv\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m agent = \u001b[43mcreate_pandas_dataframe_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dangerous_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAgentType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOPENAI_FUNCTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData Analysis Agent is ready. You can now ask questions about the data.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\source\\LLM-Concepts\\.venv\\Lib\\site-packages\\langchain_experimental\\agents\\agent_toolkits\\pandas\\base.py:313\u001b[39m, in \u001b[36mcreate_pandas_dataframe_agent\u001b[39m\u001b[34m(llm, df, agent_type, callback_manager, prefix, suffix, input_variables, verbose, return_intermediate_steps, max_iterations, max_execution_time, early_stopping_method, agent_executor_kwargs, include_df_in_prompt, number_of_head_rows, extra_tools, engine, allow_dangerous_code, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m     agent: Union[BaseSingleActionAgent, BaseMultiActionAgent] = RunnableAgent(\n\u001b[32m    308\u001b[39m         runnable=create_react_agent(llm, tools, prompt),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    309\u001b[39m         input_keys_arg=[\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    310\u001b[39m         return_keys_arg=[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m agent_type \u001b[38;5;129;01min\u001b[39;00m (AgentType.OPENAI_FUNCTIONS, \u001b[33m\"\u001b[39m\u001b[33mopenai-tools\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtool-calling\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     prompt = \u001b[43m_get_functions_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_df_in_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_df_in_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnumber_of_head_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumber_of_head_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m agent_type == AgentType.OPENAI_FUNCTIONS:\n\u001b[32m    321\u001b[39m         runnable = create_openai_functions_agent(\n\u001b[32m    322\u001b[39m             cast(BaseLanguageModel, llm), tools, prompt\n\u001b[32m    323\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\source\\LLM-Concepts\\.venv\\Lib\\site-packages\\langchain_experimental\\agents\\agent_toolkits\\pandas\\base.py:148\u001b[39m, in \u001b[36m_get_functions_prompt\u001b[39m\u001b[34m(df, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_functions_prompt\u001b[39m(df: Any, **kwargs: Any) -> ChatPromptTemplate:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    146\u001b[39m         _get_functions_multi_prompt(df, **kwargs)\n\u001b[32m    147\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_get_functions_single_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\source\\LLM-Concepts\\.venv\\Lib\\site-packages\\langchain_experimental\\agents\\agent_toolkits\\pandas\\base.py:119\u001b[39m, in \u001b[36m_get_functions_single_prompt\u001b[39m\u001b[34m(df, prefix, suffix, include_df_in_prompt, number_of_head_rows)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_functions_single_prompt\u001b[39m(\n\u001b[32m    111\u001b[39m     df: Any,\n\u001b[32m    112\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m     number_of_head_rows: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    117\u001b[39m ) -> ChatPromptTemplate:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m include_df_in_prompt:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m         df_head = \u001b[38;5;28mstr\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_head_rows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    120\u001b[39m         suffix = (suffix \u001b[38;5;129;01mor\u001b[39;00m FUNCTIONS_WITH_DF).format(df_head=df_head)\n\u001b[32m    121\u001b[39m     prefix = prefix \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m PREFIX_FUNCTIONS\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\source\\LLM-Concepts\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\source\\LLM-Concepts\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:2988\u001b[39m, in \u001b[36mDataFrame.to_markdown\u001b[39m\u001b[34m(self, buf, mode, index, storage_options, **kwargs)\u001b[39m\n\u001b[32m   2986\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mtablefmt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2987\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mshowindex\u001b[39m\u001b[33m\"\u001b[39m, index)\n\u001b[32m-> \u001b[39m\u001b[32m2988\u001b[39m tabulate = \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtabulate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2989\u001b[39m result = tabulate.tabulate(\u001b[38;5;28mself\u001b[39m, **kwargs)\n\u001b[32m   2990\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\source\\LLM-Concepts\\.venv\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "agent = create_pandas_dataframe_agent(\n",
    "    ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
    "    df,\n",
    "    verbose=True,\n",
    "    allow_dangerous_code=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")\n",
    "print(\"Data Analysis Agent is ready. You can now ask questions about the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ea668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_agent(question):\n",
    "    \"\"\"Function to ask questions to the agent and display the response\"\"\"\n",
    "    response = agent.run({\n",
    "        \"input\": question,\n",
    "        \"agent_scratchpad\": f\"Human: {question}\\nAI: To answer this question, I need to use Python to analyze the dataframe. I'll use the python_repl_ast tool.\\n\\nAction: python_repl_ast\\nAction Input: \",\n",
    "    })\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
