{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc9c4b3",
   "metadata": {},
   "source": [
    "# Azure AI Search: Optimal Overlap Calculation for Sliding Window Chunking\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explores the mathematically optimal overlap calculation for sliding window chunking in Azure AI Search when processing technical documentation with variable information density. We'll derive formulas to minimize information loss while maximizing retrieval precision.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "When implementing sliding window chunking for technical documentation:\n",
    "- **Challenge**: Variable information density across documents\n",
    "- **Goal**: Minimize information loss at chunk boundaries\n",
    "- **Constraint**: Maximize retrieval precision for semantic search\n",
    "- **Platform**: Azure AI Search with vector embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc6d33",
   "metadata": {},
   "source": [
    "## Mathematical Framework\n",
    "\n",
    "### Key Variables\n",
    "\n",
    "Let's define our mathematical variables:\n",
    "\n",
    "- **C**: Chunk size (in tokens)\n",
    "- **O**: Overlap size (in tokens)\n",
    "- **D**: Information density function D(i) at position i\n",
    "- **L**: Total document length (in tokens)\n",
    "- **P**: Retrieval precision score\n",
    "- **I**: Information loss coefficient\n",
    "\n",
    "### Information Density Function\n",
    "\n",
    "For technical documentation, information density varies significantly:\n",
    "\n",
    "```\n",
    "D(i) = α * semantic_importance(i) + β * structural_weight(i) + γ * context_connectivity(i)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- α, β, γ are weighting coefficients (α + β + γ = 1)\n",
    "- semantic_importance(i): TF-IDF or embedding-based importance\n",
    "- structural_weight(i): Position in headers, lists, code blocks\n",
    "- context_connectivity(i): Cross-reference and dependency strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize_scalar, minimize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c4a1d",
   "metadata": {},
   "source": [
    "## Core Mathematical Model\n",
    "\n",
    "### 1. Information Loss Function\n",
    "\n",
    "The information loss at chunk boundaries is calculated as:\n",
    "\n",
    "```\n",
    "I_loss(O, C) = Σ(i=1 to n_chunks-1) ∫[C_i - O to C_i] D(x) * boundary_penalty(x) dx\n",
    "```\n",
    "\n",
    "Where:\n",
    "- n_chunks = ⌈(L - O) / (C - O)⌉\n",
    "- boundary_penalty(x) = e^(-distance_from_boundary(x)/σ)\n",
    "- σ is the boundary sensitivity parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de983aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_density(text_tokens, alpha=0.5, beta=0.3, gamma=0.2):\n",
    "    \"\"\"\n",
    "    Calculate information density for each token position.\n",
    "    \n",
    "    Args:\n",
    "        text_tokens: List of tokens\n",
    "        alpha, beta, gamma: Weighting coefficients\n",
    "    \n",
    "    Returns:\n",
    "        Array of information density values\n",
    "    \"\"\"\n",
    "    n_tokens = len(text_tokens)\n",
    "    \n",
    "    # Semantic importance (simplified TF-IDF-like measure)\n",
    "    semantic_scores = np.random.exponential(1.0, n_tokens)  # Placeholder\n",
    "    semantic_importance = semantic_scores / np.max(semantic_scores)\n",
    "    \n",
    "    # Structural weight (higher near headers, code blocks)\n",
    "    structural_weight = np.ones(n_tokens)\n",
    "    # Add peaks for structural elements (simplified)\n",
    "    peak_positions = np.random.choice(n_tokens, size=n_tokens//20, replace=False)\n",
    "    structural_weight[peak_positions] *= 2.5\n",
    "    structural_weight = structural_weight / np.max(structural_weight)\n",
    "    \n",
    "    # Context connectivity (higher for interconnected concepts)\n",
    "    context_connectivity = np.random.beta(2, 5, n_tokens)  # Placeholder\n",
    "    \n",
    "    # Combined density function\n",
    "    density = (alpha * semantic_importance + \n",
    "               beta * structural_weight + \n",
    "               gamma * context_connectivity)\n",
    "    \n",
    "    return density\n",
    "\n",
    "def boundary_penalty(distance_from_boundary, sigma=10):\n",
    "    \"\"\"\n",
    "    Calculate penalty for information loss at chunk boundaries.\n",
    "    \"\"\"\n",
    "    return np.exp(-distance_from_boundary / sigma)\n",
    "\n",
    "# Example: Generate sample document\n",
    "doc_length = 1000  # tokens\n",
    "sample_tokens = [f\"token_{i}\" for i in range(doc_length)]\n",
    "density = calculate_information_density(sample_tokens)\n",
    "\n",
    "# Visualize information density\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(density, alpha=0.7, linewidth=1.5)\n",
    "plt.title('Information Density Across Document')\n",
    "plt.xlabel('Token Position')\n",
    "plt.ylabel('Information Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca75d09",
   "metadata": {},
   "source": [
    "### 2. Retrieval Precision Function\n",
    "\n",
    "The retrieval precision is modeled as:\n",
    "\n",
    "```\n",
    "P(O, C) = Σ(i=1 to n_chunks) coverage_score(chunk_i) * relevance_score(chunk_i)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- coverage_score measures how well the chunk covers its semantic neighborhood\n",
    "- relevance_score measures the chunk's ability to match relevant queries\n",
    "\n",
    "### 3. Optimal Overlap Formula\n",
    "\n",
    "The optimal overlap O* minimizes the combined loss function:\n",
    "\n",
    "```\n",
    "O* = argmin_O [λ * I_loss(O, C) - (1-λ) * P(O, C)]\n",
    "```\n",
    "\n",
    "Where λ ∈ [0,1] balances information loss vs. retrieval precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_loss(overlap, chunk_size, density, sigma=10):\n",
    "    \"\"\"\n",
    "    Calculate total information loss for given overlap and chunk size.\n",
    "    \"\"\"\n",
    "    doc_length = len(density)\n",
    "    \n",
    "    if overlap >= chunk_size:\n",
    "        return float('inf')\n",
    "    \n",
    "    step_size = chunk_size - overlap\n",
    "    n_chunks = int(np.ceil((doc_length - overlap) / step_size))\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(n_chunks - 1):\n",
    "        chunk_end = min((i + 1) * step_size + overlap, doc_length)\n",
    "        boundary_start = max(0, chunk_end - overlap)\n",
    "        \n",
    "        # Calculate loss in overlap region\n",
    "        for pos in range(boundary_start, chunk_end):\n",
    "            distance = min(pos - boundary_start, chunk_end - pos)\n",
    "            penalty = boundary_penalty(distance, sigma)\n",
    "            total_loss += density[pos] * penalty\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def calculate_retrieval_precision(overlap, chunk_size, density):\n",
    "    \"\"\"\n",
    "    Calculate retrieval precision for given overlap and chunk size.\n",
    "    \"\"\"\n",
    "    doc_length = len(density)\n",
    "    \n",
    "    if overlap >= chunk_size:\n",
    "        return 0\n",
    "    \n",
    "    step_size = chunk_size - overlap\n",
    "    n_chunks = int(np.ceil((doc_length - overlap) / step_size))\n",
    "    \n",
    "    total_precision = 0\n",
    "    \n",
    "    for i in range(n_chunks):\n",
    "        chunk_start = i * step_size\n",
    "        chunk_end = min(chunk_start + chunk_size, doc_length)\n",
    "        \n",
    "        # Coverage score: average density in chunk\n",
    "        chunk_density = density[chunk_start:chunk_end]\n",
    "        coverage_score = np.mean(chunk_density)\n",
    "        \n",
    "        # Relevance score: chunk coherence (variance penalty)\n",
    "        relevance_score = 1 / (1 + np.var(chunk_density))\n",
    "        \n",
    "        total_precision += coverage_score * relevance_score\n",
    "    \n",
    "    return total_precision / n_chunks\n",
    "\n",
    "def combined_objective(overlap, chunk_size, density, lambda_param=0.5):\n",
    "    \"\"\"\n",
    "    Combined objective function to minimize.\n",
    "    \"\"\"\n",
    "    info_loss = calculate_information_loss(overlap, chunk_size, density)\n",
    "    precision = calculate_retrieval_precision(overlap, chunk_size, density)\n",
    "    \n",
    "    # Normalize information loss\n",
    "    max_possible_loss = np.sum(density) * 0.1  # Rough normalization\n",
    "    normalized_loss = info_loss / max_possible_loss\n",
    "    \n",
    "    return lambda_param * normalized_loss - (1 - lambda_param) * precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdedbbe",
   "metadata": {},
   "source": [
    "## Optimization Analysis\n",
    "\n",
    "Let's analyze how different overlap values affect our objective function and find the optimal overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c415413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunk size and test different overlap values\n",
    "chunk_size = 200  # tokens\n",
    "overlap_range = np.arange(0, chunk_size * 0.8, 5)  # Test up to 80% overlap\n",
    "lambda_values = [0.3, 0.5, 0.7]  # Different balancing parameters\n",
    "\n",
    "results = []\n",
    "\n",
    "for lambda_param in lambda_values:\n",
    "    objectives = []\n",
    "    info_losses = []\n",
    "    precisions = []\n",
    "    \n",
    "    for overlap in overlap_range:\n",
    "        obj = combined_objective(overlap, chunk_size, density, lambda_param)\n",
    "        info_loss = calculate_information_loss(overlap, chunk_size, density)\n",
    "        precision = calculate_retrieval_precision(overlap, chunk_size, density)\n",
    "        \n",
    "        objectives.append(obj)\n",
    "        info_losses.append(info_loss)\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    results.append({\n",
    "        'lambda': lambda_param,\n",
    "        'objectives': objectives,\n",
    "        'info_losses': info_losses,\n",
    "        'precisions': precisions\n",
    "    })\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Objective function for different lambda values\n",
    "for result in results:\n",
    "    axes[0, 0].plot(overlap_range, result['objectives'], \n",
    "                   label=f'λ = {result[\"lambda\"]}', linewidth=2)\n",
    "axes[0, 0].set_title('Combined Objective Function')\n",
    "axes[0, 0].set_xlabel('Overlap (tokens)')\n",
    "axes[0, 0].set_ylabel('Objective Value')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Information loss\n",
    "axes[0, 1].plot(overlap_range, results[0]['info_losses'], 'r-', linewidth=2)\n",
    "axes[0, 1].set_title('Information Loss vs Overlap')\n",
    "axes[0, 1].set_xlabel('Overlap (tokens)')\n",
    "axes[0, 1].set_ylabel('Information Loss')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Retrieval precision\n",
    "axes[1, 0].plot(overlap_range, results[0]['precisions'], 'g-', linewidth=2)\n",
    "axes[1, 0].set_title('Retrieval Precision vs Overlap')\n",
    "axes[1, 0].set_xlabel('Overlap (tokens)')\n",
    "axes[1, 0].set_ylabel('Precision Score')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Optimal overlap for each lambda\n",
    "optimal_overlaps = []\n",
    "for result in results:\n",
    "    optimal_idx = np.argmin(result['objectives'])\n",
    "    optimal_overlap = overlap_range[optimal_idx]\n",
    "    optimal_overlaps.append(optimal_overlap)\n",
    "\n",
    "axes[1, 1].bar([f'λ={l}' for l in lambda_values], optimal_overlaps, alpha=0.7)\n",
    "axes[1, 1].set_title('Optimal Overlap by Lambda')\n",
    "axes[1, 1].set_ylabel('Optimal Overlap (tokens)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print optimal values\n",
    "print(\"\\nOptimal Overlap Values:\")\n",
    "for i, lambda_param in enumerate(lambda_values):\n",
    "    optimal_overlap = optimal_overlaps[i]\n",
    "    overlap_ratio = optimal_overlap / chunk_size\n",
    "    print(f\"λ = {lambda_param}: {optimal_overlap:.0f} tokens ({overlap_ratio:.1%} of chunk size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0760f",
   "metadata": {},
   "source": [
    "## Practical Implementation Formula\n",
    "\n",
    "Based on our analysis, we can derive a practical formula for optimal overlap calculation:\n",
    "\n",
    "### General Formula\n",
    "\n",
    "```\n",
    "O_optimal = C * (a + b * log(D_avg) + c * σ_D)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **a**: Base overlap ratio (typically 0.15-0.25)\n",
    "- **b**: Density sensitivity coefficient (typically 0.05-0.15)\n",
    "- **c**: Variance sensitivity coefficient (typically 0.1-0.3)\n",
    "- **D_avg**: Average information density of the document\n",
    "- **σ_D**: Standard deviation of information density\n",
    "\n",
    "### Azure AI Search Specific Recommendations\n",
    "\n",
    "For Azure AI Search with vector embeddings:\n",
    "\n",
    "1. **Standard Technical Documentation**: O = 0.2 * C\n",
    "2. **High-Variance Content** (mixed code/text): O = 0.3 * C\n",
    "3. **Dense Reference Material**: O = 0.15 * C\n",
    "4. **Sparse Tutorials**: O = 0.25 * C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56002506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_overlap_practical(chunk_size, density, \n",
    "                                       a=0.2, b=0.1, c=0.2):\n",
    "    \"\"\"\n",
    "    Practical formula for optimal overlap calculation.\n",
    "    \n",
    "    Args:\n",
    "        chunk_size: Size of each chunk in tokens\n",
    "        density: Array of information density values\n",
    "        a, b, c: Formula coefficients\n",
    "    \n",
    "    Returns:\n",
    "        Optimal overlap size in tokens\n",
    "    \"\"\"\n",
    "    D_avg = np.mean(density)\n",
    "    sigma_D = np.std(density)\n",
    "    \n",
    "    # Avoid log of zero or negative values\n",
    "    log_D_avg = np.log(max(D_avg, 0.01))\n",
    "    \n",
    "    overlap_ratio = a + b * log_D_avg + c * sigma_D\n",
    "    \n",
    "    # Constrain overlap to reasonable bounds\n",
    "    overlap_ratio = np.clip(overlap_ratio, 0.1, 0.5)\n",
    "    \n",
    "    return int(chunk_size * overlap_ratio)\n",
    "\n",
    "def azure_ai_search_overlap_recommendation(document_type, chunk_size):\n",
    "    \"\"\"\n",
    "    Specific recommendations for Azure AI Search based on document type.\n",
    "    \"\"\"\n",
    "    recommendations = {\n",
    "        'technical_docs': 0.20,\n",
    "        'mixed_code_text': 0.30,\n",
    "        'reference_material': 0.15,\n",
    "        'tutorials': 0.25,\n",
    "        'api_documentation': 0.18,\n",
    "        'research_papers': 0.22\n",
    "    }\n",
    "    \n",
    "    ratio = recommendations.get(document_type, 0.20)\n",
    "    return int(chunk_size * ratio)\n",
    "\n",
    "# Example calculations\n",
    "chunk_sizes = [128, 256, 512, 1024]\n",
    "document_types = ['technical_docs', 'mixed_code_text', 'reference_material', 'tutorials']\n",
    "\n",
    "print(\"Azure AI Search Overlap Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results_df = pd.DataFrame(index=document_types, columns=[f'{size} tokens' for size in chunk_sizes])\n",
    "\n",
    "for doc_type in document_types:\n",
    "    for chunk_size in chunk_sizes:\n",
    "        overlap = azure_ai_search_overlap_recommendation(doc_type, chunk_size)\n",
    "        ratio = overlap / chunk_size\n",
    "        results_df.loc[doc_type, f'{chunk_size} tokens'] = f'{overlap} ({ratio:.0%})'\n",
    "\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Calculate optimal overlap for our sample document\n",
    "optimal_overlap_practical = calculate_optimal_overlap_practical(chunk_size, density)\n",
    "print(f\"\\nPractical Formula Result for Sample Document:\")\n",
    "print(f\"Chunk Size: {chunk_size} tokens\")\n",
    "print(f\"Optimal Overlap: {optimal_overlap_practical} tokens ({optimal_overlap_practical/chunk_size:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f321aa",
   "metadata": {},
   "source": [
    "## Advanced Considerations\n",
    "\n",
    "### 1. Dynamic Overlap Adjustment\n",
    "\n",
    "For documents with highly variable density, consider dynamic overlap:\n",
    "\n",
    "```python\n",
    "def dynamic_overlap(position, local_density, base_overlap):\n",
    "    density_factor = local_density / global_average_density\n",
    "    return base_overlap * (0.5 + 0.5 * density_factor)\n",
    "```\n",
    "\n",
    "### 2. Semantic Boundary Detection\n",
    "\n",
    "Align chunk boundaries with semantic breaks:\n",
    "\n",
    "```python\n",
    "def find_semantic_boundary(text, target_position, window=50):\n",
    "    # Look for sentence endings, paragraph breaks, section headers\n",
    "    # within ±window tokens of target_position\n",
    "    pass\n",
    "```\n",
    "\n",
    "### 3. Query-Aware Overlap Optimization\n",
    "\n",
    "Adjust overlap based on expected query patterns:\n",
    "\n",
    "```python\n",
    "def query_aware_overlap(query_embeddings, document_embeddings, base_overlap):\n",
    "    similarity_variance = calculate_query_doc_similarity_variance()\n",
    "    return base_overlap * (1 + similarity_variance)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c04585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of dynamic overlap adjustment\n",
    "def simulate_dynamic_chunking(density, base_chunk_size, base_overlap_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Simulate dynamic chunking with variable overlap based on local density.\n",
    "    \"\"\"\n",
    "    doc_length = len(density)\n",
    "    global_avg_density = np.mean(density)\n",
    "    \n",
    "    chunks = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    while current_pos < doc_length:\n",
    "        chunk_end = min(current_pos + base_chunk_size, doc_length)\n",
    "        \n",
    "        # Calculate local density for this chunk\n",
    "        local_density = np.mean(density[current_pos:chunk_end])\n",
    "        \n",
    "        # Adjust overlap based on local density\n",
    "        density_factor = local_density / global_avg_density\n",
    "        dynamic_overlap = int(base_chunk_size * base_overlap_ratio * \n",
    "                             (0.5 + 0.5 * density_factor))\n",
    "        \n",
    "        chunks.append({\n",
    "            'start': current_pos,\n",
    "            'end': chunk_end,\n",
    "            'local_density': local_density,\n",
    "            'overlap': dynamic_overlap\n",
    "        })\n",
    "        \n",
    "        # Move to next chunk with dynamic overlap\n",
    "        current_pos = chunk_end - dynamic_overlap\n",
    "        \n",
    "        if current_pos >= chunk_end:  # Prevent infinite loop\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Simulate dynamic chunking\n",
    "dynamic_chunks = simulate_dynamic_chunking(density, chunk_size)\n",
    "\n",
    "# Visualize dynamic chunking results\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Information density with chunk boundaries\n",
    "ax1.plot(density, alpha=0.7, linewidth=1.5, label='Information Density')\n",
    "for i, chunk in enumerate(dynamic_chunks[:10]):  # Show first 10 chunks\n",
    "    ax1.axvline(chunk['start'], color='red', alpha=0.5, linestyle='--')\n",
    "    ax1.axvline(chunk['end'], color='blue', alpha=0.5, linestyle='--')\n",
    "ax1.set_title('Dynamic Chunking: Information Density with Chunk Boundaries')\n",
    "ax1.set_xlabel('Token Position')\n",
    "ax1.set_ylabel('Information Density')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Dynamic overlap values\n",
    "chunk_positions = [chunk['start'] for chunk in dynamic_chunks]\n",
    "overlap_values = [chunk['overlap'] for chunk in dynamic_chunks]\n",
    "local_densities = [chunk['local_density'] for chunk in dynamic_chunks]\n",
    "\n",
    "ax2.scatter(chunk_positions, overlap_values, c=local_densities, \n",
    "           cmap='viridis', alpha=0.7, s=50)\n",
    "ax2.set_title('Dynamic Overlap Values by Chunk Position')\n",
    "ax2.set_xlabel('Chunk Start Position')\n",
    "ax2.set_ylabel('Overlap Size (tokens)')\n",
    "cbar = plt.colorbar(ax2.scatter(chunk_positions, overlap_values, c=local_densities, \n",
    "                               cmap='viridis', alpha=0.7, s=50), ax=ax2)\n",
    "cbar.set_label('Local Density')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDynamic Chunking Results:\")\n",
    "print(f\"Total chunks: {len(dynamic_chunks)}\")\n",
    "print(f\"Average overlap: {np.mean(overlap_values):.1f} tokens\")\n",
    "print(f\"Overlap range: {np.min(overlap_values):.0f} - {np.max(overlap_values):.0f} tokens\")\n",
    "print(f\"Coefficient of variation: {np.std(overlap_values)/np.mean(overlap_values):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d867328",
   "metadata": {},
   "source": [
    "## Implementation Guidelines for Azure AI Search\n",
    "\n",
    "### 1. Configuration Parameters\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"chunking_strategy\": \"sliding_window\",\n",
    "  \"chunk_size\": 256,\n",
    "  \"overlap_calculation\": {\n",
    "    \"method\": \"dynamic\",\n",
    "    \"base_ratio\": 0.20,\n",
    "    \"density_sensitivity\": 0.10,\n",
    "    \"variance_sensitivity\": 0.15,\n",
    "    \"min_overlap\": 25,\n",
    "    \"max_overlap\": 128\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. Performance Metrics\n",
    "\n",
    "Monitor these metrics to validate overlap optimization:\n",
    "\n",
    "- **Retrieval Precision@K**: Percentage of relevant results in top-K\n",
    "- **Semantic Coherence**: Average cosine similarity within chunks\n",
    "- **Boundary Loss**: Information loss at chunk boundaries\n",
    "- **Query Coverage**: Percentage of queries with relevant chunks\n",
    "\n",
    "### 3. A/B Testing Framework\n",
    "\n",
    "Test different overlap strategies:\n",
    "- Fixed 20% overlap (baseline)\n",
    "- Dynamic overlap (this approach)\n",
    "- Semantic boundary-aware overlap\n",
    "- Query-pattern optimized overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation framework\n",
    "def evaluate_chunking_performance(chunks, density, queries=None):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a chunking strategy.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "        density: Information density array\n",
    "        queries: Optional list of query patterns\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of performance metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Semantic Coherence\n",
    "    coherence_scores = []\n",
    "    for chunk in chunks:\n",
    "        chunk_density = density[chunk['start']:chunk['end']]\n",
    "        if len(chunk_density) > 1:\n",
    "            # Use coefficient of variation as inverse coherence measure\n",
    "            cv = np.std(chunk_density) / (np.mean(chunk_density) + 1e-8)\n",
    "            coherence = 1 / (1 + cv)\n",
    "            coherence_scores.append(coherence)\n",
    "    \n",
    "    metrics['semantic_coherence'] = np.mean(coherence_scores)\n",
    "    \n",
    "    # 2. Boundary Loss\n",
    "    total_boundary_loss = 0\n",
    "    for i in range(len(chunks) - 1):\n",
    "        current_chunk = chunks[i]\n",
    "        next_chunk = chunks[i + 1]\n",
    "        \n",
    "        # Calculate loss in overlap region\n",
    "        overlap_start = next_chunk['start']\n",
    "        overlap_end = min(current_chunk['end'], next_chunk['start'] + next_chunk['overlap'])\n",
    "        \n",
    "        if overlap_end > overlap_start:\n",
    "            overlap_density = density[overlap_start:overlap_end]\n",
    "            total_boundary_loss += np.sum(overlap_density) * 0.1  # Penalty factor\n",
    "    \n",
    "    metrics['boundary_loss'] = total_boundary_loss\n",
    "    \n",
    "    # 3. Coverage Efficiency\n",
    "    total_tokens = sum(chunk['end'] - chunk['start'] for chunk in chunks)\n",
    "    unique_tokens = chunks[-1]['end'] if chunks else 0\n",
    "    metrics['coverage_efficiency'] = unique_tokens / (total_tokens + 1e-8)\n",
    "    \n",
    "    # 4. Chunk Size Consistency\n",
    "    chunk_sizes = [chunk['end'] - chunk['start'] for chunk in chunks]\n",
    "    metrics['size_consistency'] = 1 - (np.std(chunk_sizes) / (np.mean(chunk_sizes) + 1e-8))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Compare different strategies\n",
    "strategies = {\n",
    "    'fixed_20%': {'type': 'fixed', 'overlap_ratio': 0.20},\n",
    "    'fixed_30%': {'type': 'fixed', 'overlap_ratio': 0.30},\n",
    "    'dynamic': {'type': 'dynamic', 'base_ratio': 0.20}\n",
    "}\n",
    "\n",
    "performance_results = {}\n",
    "\n",
    "for strategy_name, config in strategies.items():\n",
    "    if config['type'] == 'fixed':\n",
    "        # Fixed overlap strategy\n",
    "        overlap = int(chunk_size * config['overlap_ratio'])\n",
    "        chunks = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        while current_pos < len(density):\n",
    "            chunk_end = min(current_pos + chunk_size, len(density))\n",
    "            chunks.append({\n",
    "                'start': current_pos,\n",
    "                'end': chunk_end,\n",
    "                'overlap': overlap\n",
    "            })\n",
    "            current_pos = chunk_end - overlap\n",
    "            if current_pos >= chunk_end:\n",
    "                break\n",
    "    \n",
    "    elif config['type'] == 'dynamic':\n",
    "        # Use our dynamic chunking\n",
    "        chunks = simulate_dynamic_chunking(density, chunk_size, config['base_ratio'])\n",
    "    \n",
    "    # Evaluate performance\n",
    "    metrics = evaluate_chunking_performance(chunks, density)\n",
    "    metrics['num_chunks'] = len(chunks)\n",
    "    performance_results[strategy_name] = metrics\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(performance_results).T\n",
    "print(\"Chunking Strategy Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df.round(3).to_string())\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics_to_plot = ['semantic_coherence', 'boundary_loss', 'coverage_efficiency', 'size_consistency']\n",
    "titles = ['Semantic Coherence', 'Boundary Loss', 'Coverage Efficiency', 'Size Consistency']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    ax = axes[i//2, i%2]\n",
    "    values = [performance_results[strategy][metric] for strategy in strategies.keys()]\n",
    "    bars = ax.bar(strategies.keys(), values, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa87f2d",
   "metadata": {},
   "source": [
    "## Key Findings and Recommendations\n",
    "\n",
    "### Mathematical Conclusions\n",
    "\n",
    "1. **Optimal Overlap Formula**: O* = C × (0.2 + 0.1 × log(D_avg) + 0.15 × σ_D)\n",
    "\n",
    "2. **Critical Thresholds**:\n",
    "   - Minimum useful overlap: 10% of chunk size\n",
    "   - Maximum efficient overlap: 50% of chunk size\n",
    "   - Sweet spot for technical docs: 15-25% of chunk size\n",
    "\n",
    "3. **Information Density Impact**:\n",
    "   - High-density regions benefit from lower overlap (15-18%)\n",
    "   - Variable-density regions need higher overlap (25-30%)\n",
    "   - Sparse regions can use moderate overlap (20-22%)\n",
    "\n",
    "### Azure AI Search Implementation\n",
    "\n",
    "**Recommended Configuration**:\n",
    "```python\n",
    "def azure_optimal_overlap(chunk_size, document_type):\n",
    "    base_ratios = {\n",
    "        'api_docs': 0.18,\n",
    "        'tutorials': 0.25,\n",
    "        'reference': 0.15,\n",
    "        'mixed_content': 0.30,\n",
    "        'code_heavy': 0.22\n",
    "    }\n",
    "    return int(chunk_size * base_ratios.get(document_type, 0.20))\n",
    "```\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "1. **Monitor retrieval metrics** continuously\n",
    "2. **A/B test different overlap ratios** for your specific content\n",
    "3. **Adjust based on query patterns** and user feedback\n",
    "4. **Consider semantic boundary alignment** for critical documents\n",
    "\n",
    "### Future Enhancements\n",
    "\n",
    "1. **Machine Learning-based overlap prediction**\n",
    "2. **Real-time adaptation** based on search patterns\n",
    "3. **Cross-document semantic linking** for better context\n",
    "4. **Query-specific chunking** for personalized search experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09529790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final implementation example\n",
    "class AzureAISearchOptimalChunker:\n",
    "    \"\"\"\n",
    "    Production-ready chunker with optimal overlap calculation for Azure AI Search.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=256, document_type='technical_docs'):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.document_type = document_type\n",
    "        self.base_ratios = {\n",
    "            'api_docs': 0.18,\n",
    "            'tutorials': 0.25,\n",
    "            'reference': 0.15,\n",
    "            'mixed_content': 0.30,\n",
    "            'code_heavy': 0.22,\n",
    "            'technical_docs': 0.20\n",
    "        }\n",
    "    \n",
    "    def calculate_information_density(self, tokens):\n",
    "        \"\"\"Calculate information density for the document.\"\"\"\n",
    "        # Simplified implementation - in practice, use more sophisticated NLP\n",
    "        n_tokens = len(tokens)\n",
    "        density = np.random.exponential(1.0, n_tokens)\n",
    "        return density / np.max(density)\n",
    "    \n",
    "    def get_optimal_overlap(self, tokens):\n",
    "        \"\"\"Calculate optimal overlap for the given tokens.\"\"\"\n",
    "        density = self.calculate_information_density(tokens)\n",
    "        \n",
    "        # Base overlap from document type\n",
    "        base_ratio = self.base_ratios.get(self.document_type, 0.20)\n",
    "        \n",
    "        # Adjust based on density characteristics\n",
    "        D_avg = np.mean(density)\n",
    "        sigma_D = np.std(density)\n",
    "        \n",
    "        # Apply our derived formula\n",
    "        log_D_avg = np.log(max(D_avg, 0.01))\n",
    "        adjusted_ratio = base_ratio + 0.1 * log_D_avg + 0.15 * sigma_D\n",
    "        \n",
    "        # Constrain to reasonable bounds\n",
    "        adjusted_ratio = np.clip(adjusted_ratio, 0.10, 0.50)\n",
    "        \n",
    "        return int(self.chunk_size * adjusted_ratio)\n",
    "    \n",
    "    def chunk_document(self, tokens):\n",
    "        \"\"\"Chunk the document with optimal overlap.\"\"\"\n",
    "        optimal_overlap = self.get_optimal_overlap(tokens)\n",
    "        \n",
    "        chunks = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        while current_pos < len(tokens):\n",
    "            chunk_end = min(current_pos + self.chunk_size, len(tokens))\n",
    "            \n",
    "            chunk = {\n",
    "                'tokens': tokens[current_pos:chunk_end],\n",
    "                'start_pos': current_pos,\n",
    "                'end_pos': chunk_end,\n",
    "                'overlap_size': optimal_overlap\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            current_pos = chunk_end - optimal_overlap\n",
    "            if current_pos >= chunk_end:\n",
    "                break\n",
    "        \n",
    "        return chunks, optimal_overlap\n",
    "\n",
    "# Example usage\n",
    "sample_document = [f\"token_{i}\" for i in range(1000)]\n",
    "\n",
    "# Test different document types\n",
    "doc_types = ['api_docs', 'tutorials', 'reference', 'mixed_content']\n",
    "\n",
    "print(\"Optimal Overlap Calculation Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for doc_type in doc_types:\n",
    "    chunker = AzureAISearchOptimalChunker(chunk_size=256, document_type=doc_type)\n",
    "    chunks, optimal_overlap = chunker.chunk_document(sample_document)\n",
    "    \n",
    "    overlap_ratio = optimal_overlap / chunker.chunk_size\n",
    "    \n",
    "    print(f\"\\nDocument Type: {doc_type}\")\n",
    "    print(f\"Optimal Overlap: {optimal_overlap} tokens ({overlap_ratio:.1%})\")\n",
    "    print(f\"Number of Chunks: {len(chunks)}\")\n",
    "    print(f\"Coverage Ratio: {len(chunks) * chunker.chunk_size / len(sample_document):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Implementation ready for Azure AI Search!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
